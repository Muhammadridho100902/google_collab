{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrFj+xxaUQf9Pz0gG62Tf+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Muhammadridho100902/google_collab/blob/main/Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F67yOHwUWfY6",
        "outputId": "3d1170df-3376-4924-b2fb-917ab01209b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading twitter-entity-sentiment-analysis.zip to /content\n",
            "\r  0% 0.00/1.99M [00:00<?, ?B/s]\n",
            "\r100% 1.99M/1.99M [00:00<00:00, 168MB/s]\n"
          ]
        }
      ],
      "source": [
        "! chmod 600 /content/kaggle.json\n",
        "! KAGGLE_CONFIG_DIR=/content/ kaggle datasets download -d jp797498e/twitter-entity-sentiment-analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import pickle"
      ],
      "metadata": {
        "id": "0wGtDOTzXhI6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_zip = zipfile.ZipFile('twitter-entity-sentiment-analysis.zip')\n",
        "dataset_zip.extractall()\n",
        "dataset_zip.close()"
      ],
      "metadata": {
        "id": "k9Hos_wVXjDe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getOneHotEncode(label):\n",
        "  # penggunaan lower untuk meng antisipasi label yang berbeda seperti \"Positive\" & \"positive\"\n",
        "  if label.lower() == 'positive':\n",
        "    return [1,0,0]\n",
        "  elif label.lower() == 'negative':\n",
        "    return [0,1,0]\n",
        "  else:\n",
        "    return [0,0,1]"
      ],
      "metadata": {
        "id": "vdwgEbCghjkM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords.words('english')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuLHNN70_pwm",
        "outputId": "6dfbd2fc-eb55-4deb-b1ff-4de3d311c88b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_url (text):\n",
        "  # http://google.com adalah bla bla bla\n",
        "  # https://youtube.com\n",
        "  return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "def remove_number(text):\n",
        "  # 123 saya blablabla\n",
        "  return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  words = text.split() # saya mau ke batam -> [saya, mau, ke, batam]\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    if word not in stopwords.words('english'):\n",
        "      new_text += word\n",
        "      new_text += \" \"\n",
        "\n",
        "  return new_text\n",
        "\n",
        "def lematized_text(text):\n",
        "  lematizer = WordNetLemmatizer()\n",
        "  words = text.split()\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    lematized_word = lematizer.lemmatize(word)\n",
        "    new_text += lematized_word\n",
        "    new_text += \" \"\n",
        "  return new_text\n",
        "\n",
        "def stemmed_text(text):\n",
        "  stemmer = PorterStemmer()\n",
        "  words = text.split()\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    new_text += stemmed_word\n",
        "    new_text += \" \"\n",
        "  return new_text\n",
        "\n",
        "def preprocessing (text):\n",
        "  #lowercase -> Running, running, RUNNING, RunNinG\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove url\n",
        "  text = remove_url(text)\n",
        "\n",
        "  # remove number\n",
        "  text = remove_number(text)\n",
        "\n",
        "  # remove stopwords\n",
        "  text = remove_stopwords(text)\n",
        "\n",
        "  # lematisasi\n",
        "  text = lematized_text(text)\n",
        "\n",
        "  # stemming\n",
        "  text = stemmed_text(text)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "YPpQ2tB1ACJz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "label = []\n",
        "\n",
        "with open('/content/twitter_training.csv', 'r') as file:\n",
        "  for sentence in file:\n",
        "    values = sentence.split(',')\n",
        "    text.append(values[3])\n",
        "    label.append(getOneHotEncode(values[2]))"
      ],
      "metadata": {
        "id": "NABw4XlEXnm3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[7])\n",
        "print(label[7])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAL18RC4i8H8",
        "outputId": "2ef9b3bc-e52a-4593-9c1b-5d9271071c38"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"So I spent a couple of hours doing something for fun... If you don't know that I'm a huge @ Borderlands fan and Maya is one of my favorite characters\n",
            "[1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing"
      ],
      "metadata": {
        "id": "1u0dIKmNk4rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "proses untuk mengubah teks / kata menjadi sebuah token agar bisa diterima oleh machine"
      ],
      "metadata": {
        "id": "MGf-P0-6k6iO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_avg(text_list):\n",
        "  sum = 0\n",
        "  for text in text_list:\n",
        "    word_num = len(text)\n",
        "    sum = sum + word_num\n",
        "  return sum / len(text_list) # total kata di dataset / banyak kalimat yang ada"
      ],
      "metadata": {
        "id": "IYMKwtW_HUJv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizing(text_list):\n",
        "  vocab_size = 10000\n",
        "\n",
        "  # penggunaan oov bertujuan untuk handle kata yang lebih dari word tokenizer\n",
        "  # contoh kita set vocab size di 2\n",
        "  # kita ingin membuat kata \"aku adalah budi\" menjadi token\n",
        "  # maka \"Aku\" menjadi index 1 \"adalah\" menjadi index 2 dan \"budi\" akan menjadi OOV\n",
        "  # artinya adalah oov merupakan sebuah param untuk handle sebuah kata yang lebih dari vocab size yang kita set\n",
        "\n",
        "  tokenizer = Tokenizer(num_words= vocab_size, oov_token = '<OOV>')\n",
        "  tokenizer.fit_on_texts(text_list)\n",
        "  word_index = tokenizer.word_index # mengubah kata menjadi index\n",
        "\n",
        "  max_length = get_avg(text_list)\n",
        "  max_length = int(max_length)\n",
        "  print(max_length)\n",
        "\n",
        "  squence = tokenizer.texts_to_sequences(text_list)\n",
        "\n",
        "  # pad_squence ini berguna untuk menambahkan atau memotong ketika kita menggunakan shape pada tensorflow\n",
        "  # ada dua yaitu padding dan trunct\n",
        "  ## padding untunk menambahkan\n",
        "  # ketika shapenya 4 dan value kita 3 ([1,2,3]) jika mengunakan padding matrix akan bertambah ([1,2,3,4])\n",
        "  ## trunct untuk memotong\n",
        "  # ketika shapenya 4 dan value kita 5 ([1,2,3,4,5]) jika mengunakan trunct matrix akan berkurang ([1,2,3,4])\n",
        "\n",
        "\n",
        "  # setiap kalimat memiliki jumlah kata yang berbeda. untuk handle hal tersebut, kita menggunakan maxlen\n",
        "  # agar setiap kalimat memiliki jumlah yang sama\n",
        "  # saat ini kita menggunakan 100 kata dalam setiap kalimat\n",
        "  # jika di suatu kalimat memiliki 3 kata saja\n",
        "  # maka squence akan bertambah sampai berjumlah 100\n",
        "  # akan diisi dengan nilai 0\n",
        "  # pada saat ini kita menggunakan post, karena akan menambahkan nilai 0 setelah squence sampai berjumlah 100 kata\n",
        "\n",
        "  padded_squence = pad_sequences(squence, padding = 'pre', truncating= 'pre', maxlen = max_length)\n",
        "\n",
        "  # export tokenizer\n",
        "  with open('my_tokenizer.pickle', 'wb') as file:\n",
        "    pickle.dump(tokenizer, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  return padded_squence"
      ],
      "metadata": {
        "id": "MVsOvAakk_B8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datas = tokenizing(text)\n",
        "datas.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUoPdAXanvMH",
        "outputId": "3c50a401-06e7-4d04-e658-61ce35ac8729"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(74682, 83)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(label) # labelnya diubah menjadi array terlebih dahulu"
      ],
      "metadata": {
        "id": "xQE4TtkmwZZR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    # input shape mengikuti datas.shape, karna kita ingin membuat datanya fleksibel makanya kita set None\n",
        "    # jika dilihat data diatas berjumlah 74682\n",
        "    tf.keras.layers.Embedding(10001, output_dim=100, input_length = 83),\n",
        "\n",
        "    tf.keras.layers.LSTM(100),\n",
        "\n",
        "    tf.keras.layers.Dense(100, activation = 'relu', input_shape=(None, 75)),\n",
        "\n",
        "    # output set di 3 karna akan menghasilkan antara \"Pos\", \"Neg\", \"Neu\"\n",
        "    tf.keras.layers.Dense(3, activation = 'softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "Uyrtw_ZixTdK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['acc'],\n",
        "    optimizer = 'adam'\n",
        ")"
      ],
      "metadata": {
        "id": "8yNaoWtNz9ny"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    datas,\n",
        "    labels,\n",
        "    epochs = 3\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuXYupy90Q1x",
        "outputId": "4eb60d90-8c73-4e35-85c1-4197552686e1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "2334/2334 [==============================] - 196s 83ms/step - loss: 0.6867 - acc: 0.6986\n",
            "Epoch 2/3\n",
            "2334/2334 [==============================] - 193s 82ms/step - loss: 0.4062 - acc: 0.8343\n",
            "Epoch 3/3\n",
            "2334/2334 [==============================] - 194s 83ms/step - loss: 0.2959 - acc: 0.8796\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a8e6bef5690>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('my_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rowNwr4kCIi8",
        "outputId": "710a924a-c9e8-4c10-82c5-7a2eaa1cb438"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PspBPv4cCcgC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6mQmambICcbP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fepl-pt_CcX7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RsP6ZdvVCcU1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfCXu5MBCcR9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-9sOFKWFCcO_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "nmQYMv7LCcIQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/my_tokenizer.pickle', 'rb') as file:\n",
        "  tokenizer = pickle.load(file)"
      ],
      "metadata": {
        "id": "57hnxaXUCp7a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "9c1dvdaFC7FY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('my_model.h5')\n",
        "\n",
        "with open('/content/my_tokenizer.pickle', 'rb') as file:\n",
        "  tokenizer = pickle.load(file)"
      ],
      "metadata": {
        "id": "mW_hehCpDC_9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stopwords.words('english')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def remove_url (text):\n",
        "  # http://google.com adalah bla bla bla\n",
        "  # https://youtube.com\n",
        "  return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "def remove_number(text):\n",
        "  # 123 saya blablabla\n",
        "  return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  words = text.split() # saya mau ke batam -> [saya, mau, ke, batam]\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    if word not in stopwords.words('english'):\n",
        "      new_text += word\n",
        "      new_text += \" \"\n",
        "\n",
        "  return new_text\n",
        "\n",
        "def lematized_text(text):\n",
        "  lematizer = WordNetLemmatizer()\n",
        "  words = text.split()\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    lematized_word = lematizer.lemmatize(word)\n",
        "    new_text += lematized_word\n",
        "    new_text += \" \"\n",
        "  return new_text\n",
        "\n",
        "def stemmed_text(text):\n",
        "  stemmer = PorterStemmer()\n",
        "  words = text.split()\n",
        "  new_text = \"\"\n",
        "  for word in words:\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    new_text += stemmed_word\n",
        "    new_text += \" \"\n",
        "  return new_text\n",
        "\n",
        "def preprocessing (text):\n",
        "  #lowercase -> Running, running, RUNNING, RunNinG\n",
        "  text = text.lower()\n",
        "\n",
        "  # remove url\n",
        "  text = remove_url(text)\n",
        "\n",
        "  # remove number\n",
        "  text = remove_number(text)\n",
        "\n",
        "  # remove stopwords\n",
        "  text = remove_stopwords(text)\n",
        "\n",
        "  # lematisasi\n",
        "  text = lematized_text(text)\n",
        "\n",
        "  # stemming\n",
        "  text = stemmed_text(text)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0ruYTHJE1Ua",
        "outputId": "c00a2f68-aeb8-4759-d256-dc75d628cb55"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = input('Masukan text yang mau di predict : ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnCw9WiADIE1",
        "outputId": "b461b320-83e1-4f7d-9d56-1378efb82916"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Masukan text yang mau di predict : you give me so much loved, but you make me sad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = preprocessing(text)"
      ],
      "metadata": {
        "id": "pXsortWZE8Uw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_LxIFitFMEK",
        "outputId": "5acddd82-a556-4d31-f350-637b5d526ffc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you give me so much loved, but you make me sad\n",
            "give much loved, make sad \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "print(sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMQvulZjFlS5",
        "outputId": "21cdad59-ba33-440f-e3a2-8526154e831c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[273, 124, 663, 139, 444]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences = pad_sequences(sequence, padding = 'pre', truncating= 'pre', maxlen = 83)"
      ],
      "metadata": {
        "id": "yW6aooXlF8oc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjvSywKqGMF_",
        "outputId": "b1f1b014-d8e4-4384-ce21-f6656121c76f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0 273 124 663 139 444]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.predict(padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Pfp5zdMGUf5",
        "outputId": "a1d49d58-16fe-4658-9079-6570dc17b4f4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 106ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_array = np.array(result)"
      ],
      "metadata": {
        "id": "FLoHWvOhv23a"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua-N_2LnxtIJ",
        "outputId": "c22bd48e-50bf-438f-aca7-01e13afda9e1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.09125289 0.8945227  0.01422437]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_category_index = np.argmax(output_array)"
      ],
      "metadata": {
        "id": "Db9JBig7xf8y"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mendefinisikan kategori\n",
        "categories = [\"Positive\", \"Negative\", \"Neutral\"]"
      ],
      "metadata": {
        "id": "giCXjKJSx0CJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mendapatkan kategori berdasarkan indeks\n",
        "predicted_category = categories[predicted_category_index]"
      ],
      "metadata": {
        "id": "mlRlxsC0xm66"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Predicted category: {predicted_category}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svSwO0fax4aC",
        "outputId": "a9fe0707-b24d-41bc-c03f-0127f925109a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted category: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use to WebAPP"
      ],
      "metadata": {
        "id": "Q76YkeYNDgJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def predictedSentences():\n",
        "    text = input(\"Masukan Kalimat untuk di Prediksi: \")\n",
        "    def remove_url(text):\n",
        "        # http://google.com adalah bla bla bla\n",
        "        # https://youtube.com\n",
        "        return re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    def remove_number(text):\n",
        "        # 123 saya blablabla\n",
        "        return re.sub(r'\\d+', '', text)\n",
        "\n",
        "    def remove_stopwords(text):\n",
        "        words = text.split()\n",
        "        new_text = \"\"\n",
        "        for word in words:\n",
        "            if word not in stopwords.words('english'):\n",
        "                new_text += word\n",
        "                new_text += \" \"\n",
        "\n",
        "        return new_text\n",
        "\n",
        "    def lemmatized_text(text):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        words = text.split()\n",
        "        new_text = \"\"\n",
        "        for word in words:\n",
        "            lemmatized_word = lemmatizer.lemmatize(word)\n",
        "            new_text += lemmatized_word\n",
        "            new_text += \" \"\n",
        "        return new_text\n",
        "\n",
        "    def stemmed_text(text):\n",
        "        stemmer = PorterStemmer()\n",
        "        words = text.split()\n",
        "        new_text = \"\"\n",
        "        for word in words:\n",
        "            stemmed_word = stemmer.stem(word)\n",
        "            new_text += stemmed_word\n",
        "            new_text += \" \"\n",
        "        return new_text\n",
        "\n",
        "    def preprocessing(text):\n",
        "        # lowercase -> Running, running, RUNNING, RunNinG\n",
        "        text = text.lower()\n",
        "\n",
        "        # remove url\n",
        "        text = remove_url(text)\n",
        "\n",
        "        # remove number\n",
        "        text = remove_number(text)\n",
        "\n",
        "        # remove stopwords\n",
        "        text = remove_stopwords(text)\n",
        "\n",
        "        # lemmatization\n",
        "        text = lemmatized_text(text)\n",
        "\n",
        "        # stemming\n",
        "        text = stemmed_text(text)\n",
        "\n",
        "        return text\n",
        "\n",
        "    preprocessed_text = preprocessing(text)\n",
        "    return preprocessed_text"
      ],
      "metadata": {
        "id": "7k3H9qVa27Od"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictSentiment():\n",
        "  model = load_model('my_model.h5')\n",
        "\n",
        "  with open('/content/my_tokenizer.pickle', 'rb') as file:\n",
        "    tokenizer = pickle.load(file)\n",
        "\n",
        "  preprocessed_text = predictedSentences()\n",
        "  sequence = tokenizer.texts_to_sequences([preprocessed_text])\n",
        "  padded_sequences = pad_sequences(sequence, padding = 'pre', truncating= 'pre', maxlen = 83)\n",
        "  result = model.predict(padded_sequences)\n",
        "  output_array = np.array(result)\n",
        "  predicted_category_index = np.argmax(output_array)\n",
        "  # Mendefinisikan kategori\n",
        "  categories = [\"Positive\", \"Negative\", \"Neutral\"]\n",
        "  # Mendapatkan kategori berdasarkan indeks\n",
        "  predicted_category = categories[predicted_category_index]\n",
        "  print(f\"Predicted category: {predicted_category}\")"
      ],
      "metadata": {
        "id": "FZJ0j3Kv4_PY"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictSentiment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpooXWig6bYu",
        "outputId": "45a09024-f90e-4d6c-a981-942c34ab615c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Masukan Kalimat untuk di Prediksi: you give me so much loved, but you make me sad\n",
            "1/1 [==============================] - 0s 456ms/step\n",
            "Predicted category: Negative\n"
          ]
        }
      ]
    }
  ]
}